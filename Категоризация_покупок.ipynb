{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Категоризация покупок.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "148GhOF9viVbiNG1-YbY66IuwPhtc-F46",
      "authorship_tag": "ABX9TyNQJpx91HO6RnLZNjGjmPwJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artstreet18/-/blob/main/%D0%9A%D0%B0%D1%82%D0%B5%D0%B3%D0%BE%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_%D0%BF%D0%BE%D0%BA%D1%83%D0%BF%D0%BE%D0%BA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzwL-7InjG6U"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import scipy as sp\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils import np_utils\n",
        "from keras import regularizers\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import log_loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HGsKnv0qN5d",
        "outputId": "facf0e0f-26f9-45fc-b532-38505b119b1c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "Ak3Shupq21Bt",
        "outputId": "116623e1-b6e3-4c9d-be62-60310df6bd02"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Категоризация покупок/input/train.csv')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-307a7b1cff8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/Категоризация покупок/input/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 8, saw 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P-ev3kAolzH"
      },
      "source": [
        "Первая проблема, с которой мы сталкиваемся, в данных существует два варианта разделения дробных чисел: через точку и через процент. Поэтому обычное чтение csv не работает."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtfZZ6k00-ov"
      },
      "source": [
        "pandas.errors.ParserError: Error tokenizing data. C error: Expected 1 fields in line 8, saw 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISzcKQqO1zJG"
      },
      "source": [
        "Есть несколько вариантов решения:\n",
        "\n",
        "\n",
        "1.   Выкинуть все строки в которых возникает подобная проблема(чаще всего это в напитках или молоке)\n",
        "2.   Отредактировать их отдельно в Excel и считывать другой формат\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Так как данных не так много было решено выбрать второй вариант\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84sTcpxYojmh"
      },
      "source": [
        "train = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/Категоризация покупок/input/train.xls', sheet_name='train' )\n",
        "\n",
        "\n",
        "train.fillna('', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "dimETTNb03w7",
        "outputId": "e6db3775-3769-471b-d12f-8887e5e7e2bc"
      },
      "source": [
        "train.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>check_id</th>\n",
              "      <th>name</th>\n",
              "      <th>category</th>\n",
              "      <th>price</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>*3479755 ТRUF.Конф.кр.корп.гл.вк.шок180г</td>\n",
              "      <td>Чай и сладкое</td>\n",
              "      <td>49.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>3408392 ECONTA Мешки д/мусора 30л  30шт</td>\n",
              "      <td>Для дома</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>3260497 ЯШКИНО Рулет С ВАР.СГУЩ. 200г</td>\n",
              "      <td>Чай и сладкое</td>\n",
              "      <td>39.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3300573 Пакет ПЯТЕРОЧКА 65х40см</td>\n",
              "      <td>Упаковка</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3413607 ЗЕР/СЕЛ.Сухари с изюмом 250г</td>\n",
              "      <td>Чай и сладкое</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>3221388 ШАРЛ.Печенье вафел.рассыпч.225г</td>\n",
              "      <td>Чай и сладкое</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>*97452 ПРОСТ.Кефир 3,2% 930г</td>\n",
              "      <td>Молочка</td>\n",
              "      <td>55.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>57575 MILFORD Зам.сахара доз.  650таб</td>\n",
              "      <td>Бакалея</td>\n",
              "      <td>119.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>29880 ПИСК.Ацидоб.2.2%сл.пюр-пак0.5л</td>\n",
              "      <td>Молочка</td>\n",
              "      <td>34.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>ШОКОЛАДНОЕ ЯЙЦО 20Г КИНДЕР СЮРПРИЗ ФЕРРЕ</td>\n",
              "      <td>Дети</td>\n",
              "      <td>49.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   check_id                                      name  ...  price count\n",
              "0         0  *3479755 ТRUF.Конф.кр.корп.гл.вк.шок180г  ...   49.0   2.0\n",
              "1         0   3408392 ECONTA Мешки д/мусора 30л  30шт  ...   21.0   1.0\n",
              "2         0     3260497 ЯШКИНО Рулет С ВАР.СГУЩ. 200г  ...   39.0   1.0\n",
              "3         0           3300573 Пакет ПЯТЕРОЧКА 65х40см  ...    4.0   1.0\n",
              "4         0      3413607 ЗЕР/СЕЛ.Сухари с изюмом 250г  ...   35.0   1.0\n",
              "5         0   3221388 ШАРЛ.Печенье вафел.рассыпч.225г  ...   38.0   1.0\n",
              "6         0              *97452 ПРОСТ.Кефир 3,2% 930г  ...   55.0   1.0\n",
              "7         0     57575 MILFORD Зам.сахара доз.  650таб  ...  119.0   1.0\n",
              "8         0      29880 ПИСК.Ацидоб.2.2%сл.пюр-пак0.5л  ...   34.0   1.0\n",
              "9         1  ШОКОЛАДНОЕ ЯЙЦО 20Г КИНДЕР СЮРПРИЗ ФЕРРЕ  ...   49.0   1.0\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTjKRa4_vXkh"
      },
      "source": [
        "Одной из основных проблем моделей с препроцессингом является то, что их очень сложно масштабировать, так как улучшая данные мы делаем менее гибкую модель. Поэтому в данном случае я попробую получить максимальный результат из сырых данных без дополнительных датасетов. При получении новых данных модель будет очень просто обучить на них, такую модель можно спокойно вклинивать в pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOUpw4_FokQY"
      },
      "source": [
        "Заметим, что данные достаточно разнородные. Где-то можно выделить отдельные слова, где-то нет. Однако для нас не важен порядок: «молоко свеж.» и «свеж. молоко» для нас одно и то же, поэтому стандартный Word2vec нам скорее вредит.\n",
        "\n",
        "Рассмотрим модели на двух вариантах TfidfVectorizer и CountVectorizer\n",
        "\n",
        "---\n",
        "\n",
        "P.S. Далее первый блок кода использует tfidf, а второй count "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuMmtsrZoqMH"
      },
      "source": [
        "tfidf_chars = TfidfVectorizer(analyzer='char', ngram_range=(2,7), max_features=8192)\n",
        "tfidf_words = TfidfVectorizer(ngram_range=(1,6), max_features=16384)\n",
        "count_chars = CountVectorizer(analyzer='char', ngram_range=(2,7), max_features=8192)\n",
        "count_words = CountVectorizer(ngram_range=(1,6), max_features=16384)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3uHqzxio33-"
      },
      "source": [
        "Для нас важны как отдельные слова в моделях, так и сочетания символов. Поэтому объединим оба варианта."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIxmia2vo4Zn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "2510009b-2630-4c01-ad6b-cd8215a7c98e"
      },
      "source": [
        "X = sp.sparse.hstack((tfidf_chars.fit_transform(train[\"name\"]), tfidf_words.fit_transform(train[\"name\"])))\n",
        "\n",
        "labeler = LabelEncoder()\n",
        "y = labeler.fit_transform(train.category)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0e2bd2ee19f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_chars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabeler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabeler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \"\"\"\n\u001b[1;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_char_ngrams\u001b[0;34m(self, text_document)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;34m\"\"\"Tokenize text_document into a sequence of character n-grams\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;31m# normalize white spaces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mtext_document\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_white_spaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_document\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mtext_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_document\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3KVG-q7qtu"
      },
      "source": [
        "Получаем ошибку, потому что у нас не все данные считываются как строка. Зададим строку в явном виде."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAq8KgQ87zCH"
      },
      "source": [
        "train['name'] = train['name'].apply(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40AaYzvc8CGy"
      },
      "source": [
        "tfidf_chars = TfidfVectorizer(analyzer='char', ngram_range=(2,7), max_features=8192 )\n",
        "tfidf_words = TfidfVectorizer(ngram_range=(1,6), max_features=16384)\n",
        "count_chars = CountVectorizer(analyzer='char', ngram_range=(2,7), max_features=8192)\n",
        "count_words = CountVectorizer(ngram_range=(1,6), max_features=16384)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I15ThA0IpsI_"
      },
      "source": [
        "Меняем y на categorical, чтобы input_shape совпадал далее с выходом softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op1Vg1Nb8G7i"
      },
      "source": [
        "X = sp.sparse.hstack((tfidf_chars.fit_transform(train[\"name\"]), tfidf_words.fit_transform(train[\"name\"])))\n",
        "\n",
        "labeler = LabelEncoder()\n",
        "y = labeler.fit_transform(train[\"category\"])\n",
        "y = to_categorical(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7qDlb-2jawY"
      },
      "source": [
        "Xc = sp.sparse.hstack((count_chars.fit_transform(train[\"name\"]), count_words.fit_transform(train[\"name\"])))\n",
        "\n",
        "labeler = LabelEncoder()\n",
        "yc = labeler.fit_transform(train[\"category\"])\n",
        "yc = to_categorical(yc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTvYr5469ZUf",
        "outputId": "f5b59885-49c3-458a-c6e3-977d26ed46a3"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggv79gYno6j_"
      },
      "source": [
        "В выборке не так много даных, поэтому вместо деления 70/30 придется использовать 80/20. Решения с меньшим количеством тестовой выборки значительно зависят от случайности, поэтому 20% оптимально."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5_BSjNio6sc"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbx7c8S1jzt1"
      },
      "source": [
        "Xc_train, Xc_test, yc_train, yc_test = train_test_split(Xc, yc, test_size=0.2, shuffle=True, random_state=9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9be6QudNqTe",
        "outputId": "9338f7e4-0cb0-4e1d-8743-4ed8652b5b73"
      },
      "source": [
        " y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9577, 25)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL2KpEjUo9wQ"
      },
      "source": [
        "Далее стандартная простая модель с выходом softmax для классификации. Dropout для уменьшения переобучения. Early Stopping позволяет получить достаточно хорошо обученную модель в момент ухудшения. Дли нигилирования эффекта случайности на каждом этапе выставляем терпение на 2(2 раза должно быть ухудшение) \n",
        "\n",
        "Оптимальные параметры для модели перебираются случайным образом.\n",
        "\n",
        "В качестве слоев были использованы [2048, 512, 25], [1024, 512, 25], [1024, 256, 25]\n",
        "\n",
        "Для активации [sigmoid, sigmoid, softmax], [relu, relu, softmax], [relu, sigmoid, softmax]\n",
        "\n",
        "Dropout [0.25, 0.25], [0.25, 0.15], [0.2, 0.2], [0.15, 0.15], [0.2, 0.15], [0.2, 0.1]\n",
        "\n",
        "оптимизатор был выбран Adam, как наиболее стабильный \n",
        "\n",
        "lr = [0.0001, 0.0002, 0.0003, 0.0005, 0.001, 0.002, 0.005]\n",
        "\n",
        "epochs не имеет значения, используем достаточно большой, чтобы выхоить за счёт Early Stopping\n",
        "\n",
        "batch_size подбирался исходя из lr, чтобы модель достаточно быстро отрабатывала(не более минуты на эпоху)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0oOhkRSo95o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c31e3634-dc33-44b9-a878-a351adcf2ea0"
      },
      "source": [
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, input_dim=X.shape[1], activation='sigmoid', ))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(256, activation='sigmoid'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(25, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr = 0.0002), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "epochs = 40\n",
        "batch_size = 30\n",
        "\n",
        "model = create_model()\n",
        "hist = model.fit(X_train, y_train,\n",
        "                 batch_size=batch_size,\n",
        "                 validation_data=(X_test, y_test),\n",
        "                 epochs=epochs,\n",
        "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 2.9384 - accuracy: 0.1328 - val_loss: 2.6775 - val_accuracy: 0.3442\n",
            "Epoch 2/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 2.6152 - accuracy: 0.2795 - val_loss: 2.1826 - val_accuracy: 0.5261\n",
            "Epoch 3/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 2.0966 - accuracy: 0.4984 - val_loss: 1.6632 - val_accuracy: 0.6335\n",
            "Epoch 4/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 1.5744 - accuracy: 0.6434 - val_loss: 1.2789 - val_accuracy: 0.7139\n",
            "Epoch 5/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 1.2380 - accuracy: 0.7228 - val_loss: 1.0342 - val_accuracy: 0.7651\n",
            "Epoch 6/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.9833 - accuracy: 0.7723 - val_loss: 0.8737 - val_accuracy: 0.7947\n",
            "Epoch 7/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.7938 - accuracy: 0.8133 - val_loss: 0.7668 - val_accuracy: 0.8118\n",
            "Epoch 8/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 0.6743 - accuracy: 0.8386 - val_loss: 0.6829 - val_accuracy: 0.8400\n",
            "Epoch 9/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 0.5870 - accuracy: 0.8617 - val_loss: 0.6199 - val_accuracy: 0.8392\n",
            "Epoch 10/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 0.5048 - accuracy: 0.8816 - val_loss: 0.5766 - val_accuracy: 0.8630\n",
            "Epoch 11/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.4374 - accuracy: 0.8988 - val_loss: 0.5309 - val_accuracy: 0.8681\n",
            "Epoch 12/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 0.3920 - accuracy: 0.9064 - val_loss: 0.5074 - val_accuracy: 0.8619\n",
            "Epoch 13/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.3405 - accuracy: 0.9163 - val_loss: 0.4763 - val_accuracy: 0.8780\n",
            "Epoch 14/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.2990 - accuracy: 0.9277 - val_loss: 0.4604 - val_accuracy: 0.8780\n",
            "Epoch 15/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 0.2748 - accuracy: 0.9322 - val_loss: 0.4406 - val_accuracy: 0.8860\n",
            "Epoch 16/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.2423 - accuracy: 0.9408 - val_loss: 0.4243 - val_accuracy: 0.8889\n",
            "Epoch 17/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.2074 - accuracy: 0.9497 - val_loss: 0.4056 - val_accuracy: 0.8908\n",
            "Epoch 18/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.1989 - accuracy: 0.9499 - val_loss: 0.4029 - val_accuracy: 0.8922\n",
            "Epoch 19/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 0.1732 - accuracy: 0.9576 - val_loss: 0.3905 - val_accuracy: 0.8966\n",
            "Epoch 20/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.1562 - accuracy: 0.9614 - val_loss: 0.3953 - val_accuracy: 0.8940\n",
            "Epoch 21/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.1454 - accuracy: 0.9622 - val_loss: 0.3884 - val_accuracy: 0.8995\n",
            "Epoch 22/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 0.1266 - accuracy: 0.9670 - val_loss: 0.3832 - val_accuracy: 0.9003\n",
            "Epoch 23/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 0.1069 - accuracy: 0.9745 - val_loss: 0.3878 - val_accuracy: 0.9024\n",
            "Epoch 24/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 0.1006 - accuracy: 0.9754 - val_loss: 0.3785 - val_accuracy: 0.9021\n",
            "Epoch 25/40\n",
            "365/365 [==============================] - 18s 50ms/step - loss: 0.0880 - accuracy: 0.9792 - val_loss: 0.3867 - val_accuracy: 0.9032\n",
            "Epoch 26/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.0784 - accuracy: 0.9823 - val_loss: 0.3790 - val_accuracy: 0.9024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovfnRhhkkAjP",
        "outputId": "ffd94b06-c4a6-4984-c916-b0f1b0581822"
      },
      "source": [
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, input_dim=X.shape[1], activation='sigmoid', ))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(256, activation='sigmoid'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(25, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr = 0.0002), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "epochs = 40\n",
        "batch_size = 30\n",
        "\n",
        "model = create_model()\n",
        "hist = model.fit(Xc_train, yc_train,\n",
        "                 batch_size=batch_size,\n",
        "                 validation_data=(Xc_test, yc_test),\n",
        "                 epochs=epochs,\n",
        "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 2.7296 - accuracy: 0.2292 - val_loss: 1.6010 - val_accuracy: 0.6569\n",
            "Epoch 2/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 1.4668 - accuracy: 0.6600 - val_loss: 0.9866 - val_accuracy: 0.7669\n",
            "Epoch 3/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 0.9249 - accuracy: 0.7840 - val_loss: 0.7553 - val_accuracy: 0.8075\n",
            "Epoch 4/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 0.6520 - accuracy: 0.8435 - val_loss: 0.6321 - val_accuracy: 0.8389\n",
            "Epoch 5/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 0.4943 - accuracy: 0.8832 - val_loss: 0.5501 - val_accuracy: 0.8550\n",
            "Epoch 6/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.3994 - accuracy: 0.9058 - val_loss: 0.4990 - val_accuracy: 0.8685\n",
            "Epoch 7/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.3213 - accuracy: 0.9275 - val_loss: 0.4742 - val_accuracy: 0.8710\n",
            "Epoch 8/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.2619 - accuracy: 0.9396 - val_loss: 0.4423 - val_accuracy: 0.8802\n",
            "Epoch 9/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.2082 - accuracy: 0.9522 - val_loss: 0.4221 - val_accuracy: 0.8845\n",
            "Epoch 10/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 0.1804 - accuracy: 0.9589 - val_loss: 0.4105 - val_accuracy: 0.8853\n",
            "Epoch 11/40\n",
            "365/365 [==============================] - 19s 51ms/step - loss: 0.1389 - accuracy: 0.9742 - val_loss: 0.4082 - val_accuracy: 0.8878\n",
            "Epoch 12/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.1203 - accuracy: 0.9758 - val_loss: 0.4032 - val_accuracy: 0.8886\n",
            "Epoch 13/40\n",
            "365/365 [==============================] - 18s 51ms/step - loss: 0.1048 - accuracy: 0.9777 - val_loss: 0.4017 - val_accuracy: 0.8908\n",
            "Epoch 14/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.0854 - accuracy: 0.9812 - val_loss: 0.3936 - val_accuracy: 0.8926\n",
            "Epoch 15/40\n",
            "365/365 [==============================] - 19s 53ms/step - loss: 0.0694 - accuracy: 0.9858 - val_loss: 0.3994 - val_accuracy: 0.8911\n",
            "Epoch 16/40\n",
            "365/365 [==============================] - 19s 52ms/step - loss: 0.0586 - accuracy: 0.9888 - val_loss: 0.4067 - val_accuracy: 0.8926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73yWUkC0pCH_"
      },
      "source": [
        "Практически на всех протестированных данных TdidfVectorize получает более хорошие значения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlviC5OIpEaB"
      },
      "source": [
        "Одним из лучших результатов без предварительной обработки данных был:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sIFH1nJpEi8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95070bf6-379b-4184-ecbf-b1cbe6a1a915"
      },
      "source": [
        "score = log_loss(y_test, model.predict(X_test))\n",
        "score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3790361030197505"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FiP8w1JpKJ5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}